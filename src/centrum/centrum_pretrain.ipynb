{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\pyenv\\GTCC\\KPG-RL\\HUST-NLP-Medical-MultiDocument-Summarization-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) != 'HUST-NLP-Medical-MultiDocument-Summarization-':\n",
    "    %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LEDForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'allenai/led-base-16384'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH)\n",
    "special_tokens_dict = {'additional_special_tokens': ['<doc-sep>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768, padding_idx=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(PATH).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_SEP_ = \"<doc-sep>\"\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT_Medical_Dataset(Dataset):\n",
    "    def __init__(self,tokenizer:AutoTokenizer,train_data):\n",
    "        self.data = train_data.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        sentence = self.data.at[id,'Abstracts']\n",
    "        target = self.data.at[id,'Target']\n",
    "        encoding = self.tokenizer(sentence, return_tensors='pt', padding=False, truncation=True, max_length=4096)\n",
    "        target_encoding = self.tokenizer(target, return_tensors='pt', padding=False, truncation=True, max_length=1024)\n",
    "        global_attention_mask = [[1 if y in [tokenizer.cls_token_id, docsep_token_id] else 0 for y in x]\n",
    "                                                 for x in encoding['input_ids']]\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0), # Squeeze to remove the extra dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': target_encoding['input_ids'].squeeze(0),\n",
    "            'global_attention_mask': torch.tensor(global_attention_mask).squeeze(0),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "class Medical_Dataset(Dataset):\n",
    "    def __init__(self,tokenizer:AutoTokenizer,train_data,train_label):\n",
    "        self.data = train_data\n",
    "        self.label = train_label\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        sentence = self.data.at[id,'Abstracts']\n",
    "        target = self.label.at[id,'Target']\n",
    "        encoding = self.tokenizer(sentence, return_tensors='pt', truncation=True, max_length=4096)\n",
    "        target_encoding = self.tokenizer(target, return_tensors='pt', truncation=True, max_length=1024)\n",
    "        global_attention_mask = [[1 if y in [tokenizer.cls_token_id, docsep_token_id] else 0 for y in x]\n",
    "                                                 for x in encoding['input_ids']]\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0), # Squeeze to remove the extra dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': target_encoding['input_ids'].squeeze(0),\n",
    "            'global_attention_mask': torch.tensor(global_attention_mask).squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochrane_train_input = pd.read_csv(\"./datasets/mslr_data/ms2/train-inputs-pretrain.csv\")\n",
    "cochrane_train_input = cochrane_train_input.iloc[0:1,:]\n",
    "\n",
    "train_dataset = PT_Medical_Dataset(tokenizer,cochrane_train_input)\n",
    "\n",
    "\n",
    "cochrane_dev_input = pd.read_csv(\".\\datasets\\mslr_data\\ms2\\dev-inputs.csv\")\n",
    "cochrane_dev_input[\"Abstract\"].fillna(\"\",inplace = True)\n",
    "cochrane_dev_input = cochrane_dev_input.groupby('ReviewID').apply(lambda group:\n",
    "    \"\".join([f\"{row['Title']}{DOC_SEP_}{row['Abstract']}{DOC_SEP_}\" for index, row in group.iterrows()])\n",
    ").reset_index(name=\"Abstracts\")\n",
    "cochrane_dev_label = pd.read_csv(\".\\datasets\\mslr_data\\ms2\\dev-targets.csv\")\n",
    "\n",
    "cochrane_dev_input.sort_values(by='ReviewID', inplace=True)\n",
    "cochrane_dev_input.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cochrane_dev_label.drop_duplicates(subset=['ReviewID'], keep='first', inplace=True)\n",
    "cochrane_dev_label.sort_values(by='ReviewID', inplace=True)\n",
    "cochrane_dev_label.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cochrane_dev_input = cochrane_dev_input.iloc[0:100,:]\n",
    "cochrane_dev_label = cochrane_dev_label.iloc[0:100,:]\n",
    "\n",
    "valid_dataset = Medical_Dataset(tokenizer,cochrane_dev_input,cochrane_dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api_key = \"b837839166bd4f97a07e90a26fa965ee17f8b64f\"\n",
    "wandb.login(key=api_key)\n",
    "wandb.init(project = \"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        print(input_ids.shape)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, global_attention_mask=global_attention_mask)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_train_loss += loss.item() \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "    wandb.log({\"epoch\": epoch+1, \"avg_train_loss\": avg_train_loss})\n",
    "\n",
    "        \n",
    "    model.eval()\n",
    "    total_valid_loss = 0 \n",
    "    predictions = [] \n",
    "    references = [] \n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, global_attention_mask=global_attention_mask)\n",
    "        loss = outputs.loss\n",
    "        total_valid_loss += loss.item()\n",
    "\n",
    "        filtered_labels = []\n",
    "        for label_sequence in labels:\n",
    "            filtered_sequence = label_sequence[label_sequence != -100] # Keep only IDs != -100\n",
    "            filtered_labels.append(filtered_sequence)\n",
    "        \n",
    "        predicted_tokens = outputs.logits.argmax(dim=-1) \n",
    "        decoded_preds = tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True) \n",
    "        decoded_labels = tokenizer.batch_decode(filtered_labels, skip_special_tokens=True) \n",
    "        \n",
    "\n",
    "        predictions.extend(decoded_preds) \n",
    "        references.extend(decoded_labels)\n",
    "\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "\n",
    "\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    wandb.log(rouge_results) \n",
    "\n",
    "    bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    wandb.log(bertscore_results) \n",
    "    \n",
    "    wandb.log({\"avg_valid_loss\": avg_valid_loss}) \n",
    "    print(f\"Epoch {epoch+1} completed, Avg. Train Loss: {avg_train_loss:.4f}, Avg. Valid Loss: {avg_valid_loss:.4f}\")\n",
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_save_path = \"my_centrum_tokenizer\"\n",
    "model_save_path = \"my_centrum_led_model\"\n",
    "\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "model.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
